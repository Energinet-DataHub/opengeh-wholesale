# Copyright 2020 Energinet DataHub A/S
#
# Licensed under the Apache License, Version 2.0 (the "License2");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: CI Databricks

on:
  workflow_call:
    inputs:
      has_calculator_job_changes:
        description: Whether there are changes in the calculator job folder
        required: true
        type: boolean
      has_settlement_report_job_changes:
        description: Whether there are changes in the settlement report job folder
        required: true
        type: boolean
      has_delta_migration_changes:
        description: Whether there are changes in the delta migration scripts
        required: true
        type: boolean
      image_tag:
        type: string
        default: latest

jobs:
  databricks_ci_build:
    uses: Energinet-DataHub/.github/.github/workflows/databricks-build-prerelease.yml@v14
    with:
      python_version: 3.11.7
      architecture: x64
      wheel_working_directory: ./source/databricks
      multiple_wheels: true
      should_include_assets: true

  migration_scripts_verification:
    if: ${{ inputs.has_delta_migration_changes }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout current branch
        uses: actions/checkout@v4

      - name: Get files from PR branch
        id: get_script_files_in_pr
        run: |
          # Gets all files in migration_scripts folder on the branch being merged excluding the __init__ file and put them into at comma seperated string
          SCRIPT_FILES_IN_PR=$(find ./source/databricks/calculation_engine/package/datamigration/migration_scripts -type f -not -name '__init__*' -printf "%f,")
          echo "::set-output name=script_files_in_pr::$SCRIPT_FILES_IN_PR"

      - name: Checkout main branch
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Get files from main branch
        id: get_script_files_in_main
        run: |
          # Gets all files in migration_scripts folder on the main branch excluding the __init__ file and put them into at comma seperated string
          SCRIPT_FILES_IN_MAIN=$(find ./source/databricks/calculation_engine/package/datamigration/migration_scripts -type f -name '*.sql' -printf "%f,")
          echo "::set-output name=script_files_in_main::$SCRIPT_FILES_IN_MAIN"

      - name: Verify migration scripts
        run: |
          # This step verifies:
          # 1. That all files in main branch exists in the PR branch TODO JMG: remove this check when we the pipeline supports deleting scripts
          # 2. The timestamp of the new scripts in the PR branch is greater than the latest script in the main branch
          # 3. The timestamp of the new scripts in the PR branch  in the PR branch is less than the current datetime
          # 4. The new script filenames in the PR branch have an underscore at the 13th character (assuming the first 12 are YYYYMMDDHHMM)

          CURRENT_DATETIME=$(TZ=Europe/Paris date +"%Y-%m-%d %H:%M:%S")
          #Internal Field Seperator
          IFS=','

          # Get the comma seperated string saved to output script_files_in_pr
          SCRIPT_FILES_IN_PR="${{ steps.get_script_files_in_pr.outputs.script_files_in_pr }}"
          # Turns comma seperated string in to an array using the IFS
          read -ra SCRIPT_FILES_IN_PR_ARRAY <<<"$SCRIPT_FILES_IN_PR"

          # Get the comma separated string saved to output script_files_in_main
          SCRIPT_FILES_IN_MAIN="${{ steps.get_script_files_in_main.outputs.script_files_in_main }}"
          # Turns comma seperated string in to an array using the IFS
          read -ra SCRIPT_FILES_IN_MAIN_ARRAY <<<"$SCRIPT_FILES_IN_MAIN"

          # Reverse sort the SCRIPT_FILES_IN_MAIN_ARRAY to get the file with the latest prefix datetime first
          script_files_in_main_reverse_sorted=($(echo "${SCRIPT_FILES_IN_MAIN_ARRAY[@]}" | tr ' ' '\n' | sort -r))
          # Turn script_files_in_main_reverse_sorted into a array (the array will only have one item, since it wont look past a new line)
          read -ra LATEST_SCRIPT_FILE_IN_MAIN <<<"$script_files_in_main_reverse_sorted"

          # Check if there are no script files in main
          if [ ${#SCRIPT_FILES_IN_MAIN_ARRAY[@]} -eq 0 ]; then
            echo "No script files found in the main directory. Skipping step."
            exit 0
          fi

          # Create a datetime from the prefix of the first item in the LATEST_SCRIPT_FILE_IN_MAIN array
          LATEST_SCRIPT_FILE_IN_MAIN_DATETIME=$(date -d "${LATEST_SCRIPT_FILE_IN_MAIN[0]:0:8} ${LATEST_SCRIPT_FILE_IN_MAIN[0]:8:2}:${LATEST_SCRIPT_FILE_IN_MAIN[0]:10:2}" +"%Y-%m-%d %H:%M:%S")

          for item in "${SCRIPT_FILES_IN_PR_ARRAY[@]}"
          do
            # If a item from the SCRIPT_FILES_IN_PR_ARRAY does not exist in the SCRIPT_FILES_IN_MAIN_ARRAY then
            if [[ ! " ${SCRIPT_FILES_IN_MAIN_ARRAY[@]} " =~ "${item}" ]]; then
              # Create a datetime from the prefix of the item that was not in the SCRIPT_FILES_IN_MAIN_ARRAY
              UNIQUE_SCRIPT_FILE_IN_PR_DATETIME=$(date -d "${item:0:8} ${item:8:2}:${item:10:2}" +"%Y-%m-%d %H:%M:%S")
              # Fail if the items datetime is less than the LATEST_SCRIPT_FILE_IN_MAIN_DATETIME
              if [[ $UNIQUE_SCRIPT_FILE_IN_PR_DATETIME < $LATEST_SCRIPT_FILE_IN_MAIN_DATETIME ]]; then
                echo "New migration script file: ${item} can not be before the latest migration script"
                exit 1
              fi
              # Fail if the items datetime is equal the LATEST_SCRIPT_FILE_IN_MAIN_DATETIME
              if [[ $UNIQUE_SCRIPT_FILE_IN_PR_DATETIME == $LATEST_SCRIPT_FILE_IN_MAIN_DATETIME ]]; then
                echo "New migration script file: ${item} can not be the same time as the latest migration script"
                exit 1
              fi
              # Fail if the items datetime is greater than the CURRENT_DATETIME
              if [[ $UNIQUE_SCRIPT_FILE_IN_PR_DATETIME > $CURRENT_DATETIME ]]; then
                echo "New migration script file: ${item} can not be later than current datetime"
                exit 1
              fi
              # Fail if the first character after the datetime is not an underscore
              if [[ "${item:12:1}" != "_" ]]; then
                echo "New migration script file: ${item} does not have an underscore after the datetime"
                exit 1
              fi
            fi
          done

  # Calculator job tests that do not require the integration test environment
  unit_tests:
    if: ${{ inputs.has_calculator_job_changes }}
    strategy:
      fail-fast: false
      matrix:
        # IMPORTANT: When adding a new folder here it should also be added in the `unit_test_check` job!
        tests_filter_expression:
          - name: Energy (calculator_job)
            paths: calculator_job/test_energy_calculation.py
          - name: Wholesale (calculator_job)
            paths: calculator_job/test_wholesale_calculation.py
          - name: Energy logic
            paths: features/energy_calculation
          - name: Wholesale logic
            paths: features/wholesale_calculation
          - name: Data products
            paths: features/data_products contracts
          - name: Unit tests
            paths: calculation codelists common optimize_job databases infrastructure
          - name: Entry point tests
            paths: entry_points/
    uses: Energinet-DataHub/.github/.github/workflows/python-ci.yml@v14
    with:
      job_name: ${{ matrix.tests_filter_expression.name }}
      operating_system: dh3-ubuntu-20.04-4core
      path_static_checks: ./source/databricks/calculation_engine
      # documented here: https://github.com/Energinet-DataHub/opengeh-wholesale/tree/main/source/databricks#styling-and-formatting
      ignore_errors_and_warning_flake8: E501,F401,E402,E203,W503
      tests_folder_path: ./source/databricks/calculation_engine/tests
      test_report_path: ./source/databricks/calculation_engine/tests
      # See .docker/entrypoint.py on how to use the filter expression
      tests_filter_expression: ${{ matrix.tests_filter_expression.paths }}
      image_tag: ${{ inputs.image_tag }}

  # Tests that require the integration test environment
  calculator_job_integration_tests:
    if: ${{ inputs.has_calculator_job_changes }}
    uses: Energinet-DataHub/.github/.github/workflows/python-ci.yml@v14
    with:
      operating_system: dh3-ubuntu-20.04-4core
      path_static_checks: ./source/databricks/calculation_engine
      # documented here: https://github.com/Energinet-DataHub/opengeh-wholesale/tree/main/source/databricks#styling-and-formatting
      ignore_errors_and_warning_flake8: E501,F401,E402,E203,W503
      tests_folder_path: ./source/databricks/calculation_engine/tests
      test_report_path: ./source/databricks/calculation_engine/tests
      # See .docker/entrypoint.py on how to use the filter expression
      tests_filter_expression: calculator_job/test_start.py calculator_job/test_start_with_deps.py
      use_integrationtest_environment: true
      azure_integrationtest_tenant_id: ${{ vars.integration_test_azure_tenant_id }}
      azure_integrationtest_subscription_id: ${{ vars.integration_test_azure_subscription_id }}
      azure_integrationtest_spn_id: ${{ vars.integration_test_azure_spn_id_oidc }}
      azure_keyvault_url: ${{ vars.integration_test_azure_keyvault_url }}
      image_tag: ${{ inputs.image_tag }}

  # Calculator job migration tests
  calculator_job_migrations_tests:
    if: ${{ inputs.has_delta_migration_changes }}
    strategy:
      fail-fast: false
      matrix:
        # IMPORTANT: When adding a new folder here it should also be added in the `unit_test_check` job!
        tests_filter_expression:
          - name: Test Unity Catalog
            paths: datamigration/unity_catalog
    uses: Energinet-DataHub/.github/.github/workflows/python-ci.yml@v14
    with:
      job_name: ${{ matrix.tests_filter_expression.name }}
      operating_system: dh3-ubuntu-20.04-4core
      path_static_checks: ./source/databricks/calculation_engine
      # documented here: https://github.com/Energinet-DataHub/opengeh-wholesale/tree/main/source/databricks#styling-and-formatting
      ignore_errors_and_warning_flake8: E501,F401,E402,E203,W503
      tests_folder_path: ./source/databricks/calculation_engine/tests
      test_report_path: ./source/databricks/calculation_engine/tests
      # See .docker/entrypoint.py on how to use the filter expression
      tests_filter_expression: ${{ matrix.tests_filter_expression.paths }} # source/databricks/calculation_engine/tests/
      image_tag: ${{ inputs.image_tag }}

  # Check executed unit tests
  calculator_job_unit_test_check:
    if: ${{ inputs.has_calculator_job_changes }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ github.token }}

      - name: Execute python tests
        shell: bash
        id: test_count
        run: |
          # Small hack to get the repository name
          repository=${{ github.repository }}
          repository_owner=${{ github.repository_owner }}
          repository_name=${repository/$repository_owner\//}

          # IMPORTANT: When adding a new folder here, one must also add the folder
          # to one of the test jobs above! This is because this filter contains the sum of all folders
          # from test jobs.

          filter="calculator_job/test_energy_calculation.py
          calculator_job/test_wholesale_calculation.py
          calculator_job/test_start.py
          calculator_job/test_start_with_deps.py
          features/energy_calculation
          features/wholesale_calculation
          features/data_products
          calculation/
          codelists/
          common/
          contracts/
          databases/
          infrastructure/
          entry_points/
          optimize_job/
          datamigration/unity_catalog"

          IMAGE_TAG=${{ inputs.image_tag }} docker compose -f .devcontainer/docker-compose.yml run --rm -u root -w //workspaces/${repository_name} python-unit-test ./.devcontainer/check_test_count.sh $filter

  calculator_job_mypy_check:
    if: ${{ inputs.has_calculator_job_changes }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: 3.x
      - name: Run pip intall and mypy check of files in package
        shell: bash
        run: |
          pip install --upgrade pip
          pip install mypy types-python-dateutil
          mypy ./source/databricks/calculation_engine/package --disallow-untyped-defs --ignore-missing-imports

  calculator_job_black_check:
    if: ${{ inputs.has_calculator_job_changes }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: psf/black@stable
        with:
          options: --check --diff
          src: ./source/databricks/calculation_engine

  settlement_report_unit_tests:
    if: ${{ inputs.has_settlement_report_job_changes }}
    strategy:
      fail-fast: false
      matrix:
        # IMPORTANT: When adding a new folder here it should also be added in the `unit_test_check` job!
        tests_filter_expression:
          - name: Settlement Report Testing
            paths: data_seeding/ domain/ entry_points/ infrastructure/ test_settlement_report_job_utils.py
    uses: Energinet-DataHub/.github/.github/workflows/python-ci.yml@v14
    with:
      operating_system: dh3-ubuntu-20.04-4core
      path_static_checks: ./source/databricks/settlement_report
      # documented here: https://github.com/Energinet-DataHub/opengeh-wholesale/tree/main/source/databricks#styling-and-formatting
      ignore_errors_and_warning_flake8: E501,F401,E402,E203,W503
      tests_folder_path: ./source/databricks/settlement_report/tests
      test_report_path: ./source/databricks/settlement_report/tests
      # See .docker/entrypoint.py on how to use the filter expression
      tests_filter_expression: ${{ matrix.tests_filter_expression.paths }}
      image_tag: ${{ inputs.image_tag }}

  # Tests that require the integration test environment
  settlement_report_integration_tests:
    if: ${{ inputs.has_settlement_report_job_changes }}
    uses: Energinet-DataHub/.github/.github/workflows/python-ci.yml@v14
    with:
      operating_system: dh3-ubuntu-20.04-4core
      path_static_checks: ./source/databricks/settlement_report
      # documented here: https://github.com/Energinet-DataHub/opengeh-wholesale/tree/main/source/databricks#styling-and-formatting
      ignore_errors_and_warning_flake8: E501,F401,E402,E203,W503
      tests_folder_path: ./source/databricks/settlement_report/tests
      test_report_path: ./source/databricks/settlement_report/tests
      # See .docker/entrypoint.py on how to use the filter expression
      tests_filter_expression: integration_test/
      use_integrationtest_environment: true
      azure_integrationtest_tenant_id: ${{ vars.integration_test_azure_tenant_id }}
      azure_integrationtest_subscription_id: ${{ vars.integration_test_azure_subscription_id }}
      azure_integrationtest_spn_id: ${{ vars.integration_test_azure_spn_id_oidc }}
      azure_keyvault_url: ${{ vars.integration_test_azure_keyvault_url }}
      image_tag: ${{ inputs.image_tag }}

  # Check executed unit tests
  settlement_report_unit_test_check:
    if: ${{ inputs.has_settlement_report_job_changes }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ github.token }}

      - name: Execute python tests
        shell: bash
        id: test_count
        run: |
          # Small hack to get the repository name
          repository=${{ github.repository }}
          repository_owner=${{ github.repository_owner }}
          repository_name=${repository/$repository_owner\//}

          # IMPORTANT: When adding a new folder here, one must also add the folder
          # to one of the test jobs above! This is because this filter contains the sum of all folders
          # from test jobs.
          filter="data_seeding/
          domain/
          entry_points/
          infrastructure/
          test_settlement_report_job_utils.py"

          IMAGE_TAG=${{ inputs.image_tag }} docker compose -f .devcontainer/docker-compose.yml run --rm -u root -w //workspaces/${repository_name} python-unit-test ./.devcontainer/check_test_count.sh $filter

  settlement_report_mypy_check:
    if: ${{ inputs.has_settlement_report_job_changes }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: 3.x
      - name: Run pip intall and mypy check of files in package
        shell: bash
        run: |
          pip install --upgrade pip
          pip install mypy types-python-dateutil
          mypy ./source/databricks/settlement_report/settlement_report_job --disallow-untyped-defs --ignore-missing-imports

  settlement_report_black_check:
    if: ${{ inputs.has_settlement_report_job_changes }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: psf/black@stable
        with:
          options: --check --diff
          src: ./source/databricks/settlement_report
