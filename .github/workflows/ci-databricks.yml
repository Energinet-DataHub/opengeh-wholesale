# Copyright 2020 Energinet DataHub A/S
#
# Licensed under the Apache License, Version 2.0 (the "License2");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: CI Databricks

on:
  workflow_call:
    inputs:
      image_tag:
        type: string
        default: latest

jobs:
  databricks_ci_build:
    uses: Energinet-DataHub/.github/.github/workflows/databricks-build-prerelease.yml@v13
    with:
      python_version: 3.11.7
      architecture: x64
      wheel_working_directory: ./source/databricks/calculation_engine
      should_include_assets: false  # Functionality is deprecated

  migration_scripts_verification:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout current branch
        uses: actions/checkout@v4

      - name: Get files from PR branch
        id: get_script_files_in_pr
        run: |
          # Gets all files in migration_scripts folder on the branch being merged excluding the __init__ file and put them into at comma seperated string
          SCRIPT_FILES_IN_PR=$(find ./source/databricks/calculation_engine/package/datamigration/migration_scripts -type f -not -name '__init__*' -printf "%f,")
          echo "::set-output name=script_files_in_pr::$SCRIPT_FILES_IN_PR"

      - name: Checkout main branch
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Get files from main branch
        id: get_script_files_in_main
        run: |
          # Gets all files in migration_scripts folder on the main branch excluding the __init__ file and put them into at comma seperated string
          SCRIPT_FILES_IN_MAIN=$(find ./source/databricks/calculation_engine/package/datamigration/migration_scripts -type f -name '*.sql' -printf "%f,")
          echo "::set-output name=script_files_in_main::$SCRIPT_FILES_IN_MAIN"

      - name: Verify migration script names
        run: |
          CURRENT_DATETIME=$(TZ=Europe/Paris date +"%Y-%m-%d %H:%M:%S")
          #Internal Field Seperator
          IFS=','

          # Get the comma seperated string saved to output script_files_in_pr
          SCRIPT_FILES_IN_PR="${{ steps.get_script_files_in_pr.outputs.script_files_in_pr }}"
          # Turns comma seperated string in to an array using the IFS
          read -ra SCRIPT_FILES_IN_PR_ARRAY <<<"$SCRIPT_FILES_IN_PR"

          # Get the comma separated string saved to output script_files_in_main
          SCRIPT_FILES_IN_MAIN="${{ steps.get_script_files_in_main.outputs.script_files_in_main }}"
          # Turns comma seperated string in to an array using the IFS
          read -ra SCRIPT_FILES_IN_MAIN_ARRAY <<<"$SCRIPT_FILES_IN_MAIN"

          # Reverse sort the SCRIPT_FILES_IN_MAIN_ARRAY to get the file with the latest prefix datetime first
          script_files_in_main_reverse_sorted=($(echo "${SCRIPT_FILES_IN_MAIN_ARRAY[@]}" | tr ' ' '\n' | sort -r))
          # Turn script_files_in_main_reverse_sorted into a array (the array will only have one item, since it wont look past a new line)
          read -ra LATEST_SCRIPT_FILE_IN_MAIN <<<"$script_files_in_main_reverse_sorted"

          # Check if there are no script files in main
          if [ ${#SCRIPT_FILES_IN_MAIN_ARRAY[@]} -eq 0 ]; then
            echo "No script files found in the main directory. Skipping step."
            exit 0
          fi

          # Create a datetime from the prefix of the first item in the LATEST_SCRIPT_FILE_IN_MAIN array
          LATEST_SCRIPT_FILE_IN_MAIN_DATETIME=$(date -d "${LATEST_SCRIPT_FILE_IN_MAIN[0]:0:8} ${LATEST_SCRIPT_FILE_IN_MAIN[0]:8:2}:${LATEST_SCRIPT_FILE_IN_MAIN[0]:10:2}" +"%Y-%m-%d %H:%M:%S")

          for item in "${SCRIPT_FILES_IN_PR_ARRAY[@]}"
          do
            # If a item from the SCRIPT_FILES_IN_PR_ARRAY does not exist in the SCRIPT_FILES_IN_MAIN_ARRAY then
            if [[ ! " ${SCRIPT_FILES_IN_MAIN_ARRAY[@]} " =~ "${item}" ]]; then
              # Create a datetime from the prefix of the item that was not in the SCRIPT_FILES_IN_MAIN_ARRAY
              UNIQUE_SCRIPT_FILE_IN_PR_DATETIME=$(date -d "${item:0:8} ${item:8:2}:${item:10:2}" +"%Y-%m-%d %H:%M:%S")
              # Fail if the items datetime is less than the LATEST_SCRIPT_FILE_IN_MAIN_DATETIME
              if [[ $UNIQUE_SCRIPT_FILE_IN_PR_DATETIME < $LATEST_SCRIPT_FILE_IN_MAIN_DATETIME ]]; then
                echo "New migration script file: ${item} can not be before the latest migration script"
                exit 1
              fi
              # Fail if the items datetime is equal the LATEST_SCRIPT_FILE_IN_MAIN_DATETIME
              if [[ $UNIQUE_SCRIPT_FILE_IN_PR_DATETIME == $LATEST_SCRIPT_FILE_IN_MAIN_DATETIME ]]; then
                echo "New migration script file: ${item} can not be the same time as the latest migration script"
                exit 1
              fi
              # Fail if the items datetime is greater than the CURRENT_DATETIME
              if [[ $UNIQUE_SCRIPT_FILE_IN_PR_DATETIME > $CURRENT_DATETIME ]]; then
                echo "New migration script file: ${item} can not be later than current datetime"
                exit 1
              fi
            fi
          done

  python_tests_calculationjob:
    uses: Energinet-DataHub/.github/.github/workflows/python-ci.yml@v13
    with:
      operating_system: dh3-ubuntu-20.04-4core
      path_static_checks: ./source/databricks/calculation_engine
      # documented here: https://github.com/Energinet-DataHub/opengeh-wholesale/tree/main/source/databricks#styling-and-formatting
      ignore_errors_and_warning_flake8: E501,F401,E402,E203,W503
      test_report_path: ./source/databricks/calculation_engine/tests
      tests_filter_expression: calculator_job
      use_integrationtest_environment: true
      azure_integrationtest_tenant_id: ${{ vars.integration_test_azure_tenant_id }}
      azure_integrationtest_subscription_id: ${{ vars.integration_test_azure_subscription_id }}
      azure_integrationtest_spn_id: ${{ vars.integration_test_azure_spn_id_oidc }}
      azure_keyvault_url: ${{ vars.integration_test_azure_keyvault_url }}
      image_tag: ${{ inputs.image_tag }}

  python_tests_other:
    uses: Energinet-DataHub/.github/.github/workflows/python-ci.yml@v13
    with:
      operating_system: dh3-ubuntu-20.04-4core
      path_static_checks: ./source/databricks/calculation_engine
      # documented here: https://github.com/Energinet-DataHub/opengeh-wholesale/tree/main/source/databricks#styling-and-formatting
      ignore_errors_and_warning_flake8: E501,F401,E402,E203,W503
      test_report_path: ./source/databricks/calculation_engine/tests
      tests_filter_expression: not calculator_job
      image_tag: ${{ inputs.image_tag }}

  mypy_check:
    runs-on: ubuntu-latest
    name: Static type checker
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: 3.x
      - name: Run pip intall and mypy check of files in package
        shell: bash
        run: |
          pip install --upgrade pip
          pip install mypy types-python-dateutil
          mypy ./source/databricks/calculation_engine/package --disallow-untyped-defs --ignore-missing-imports

  black_check:
    runs-on: ubuntu-latest
    name: Code style check
    steps:
      - uses: actions/checkout@v4
      - uses: psf/black@stable
        with:
          options: --check --diff
          src: ./source/databricks/calculation_engine
