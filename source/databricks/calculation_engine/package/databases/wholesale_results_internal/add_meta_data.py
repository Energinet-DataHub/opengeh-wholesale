# Copyright 2020 Energinet DataHub A/S
#
# Licensed under the Apache License, Version 2.0 (the "License2");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, lit

from package.calculation.calculator_args import CalculatorArgs
from package.constants import Colname
from pyspark.sql.types import StringType


def add_metadata(
    args: CalculatorArgs,
    column_group_for_calculation_result_id: list[str],
    df: DataFrame,
) -> DataFrame:
    df = (
        df.withColumn(Colname.calculation_id, lit(args.calculation_id))
        .withColumn(Colname.calculation_type, lit(args.calculation_type.value))
        .withColumn(
            Colname.calculation_execution_time_start,
            lit(args.calculation_execution_time_start),
        )
    )
    df = _add_calculation_result_id(df, column_group_for_calculation_result_id)

    return df


def _add_calculation_result_id(
    df: DataFrame,
    column_group_for_calculation_result_id: list[str],
) -> DataFrame:
    """
    The Databricks engine is not fond of non-deterministic functions like 'uuid()' when it comes to retry.
    Therefore, the result id is now generated by concatenating calculation id and values of selected
    partition columns, and converting the concatenated value into a deterministic uuid using uuid5.

    First the concatenated value is created in a new column as a string type, then the uuid5 is calculated
    and stored in the calculation_result_id column. The new column is then dropped.
    """

    # The imports must be added locally in order to be imported correctly.
    import uuid
    from package.databases.table_column_names import TableColumnNames
    import pyspark.sql.functions as f

    namespace = uuid.UUID("681fd884-0a2e-4dc2-96ea-c61c3683449c")

    def _generate_uuid5(concatenated_str: str) -> str:
        return str(uuid.uuid5(namespace, concatenated_str))

    # User defined function to generate a deterministic uuid5 from a string
    uuid5_udf = f.udf(_generate_uuid5, StringType())

    concat_placeholder = "concat_placeholder"
    df = df.withColumn(
        concat_placeholder,
        f.concat_ws("", *[col(c) for c in column_group_for_calculation_result_id]),
    )

    df = df.withColumn(
        # TODO AJW: Rename to result_id when we are on Unity Catalog.
        TableColumnNames.calculation_result_id,
        uuid5_udf(col(concat_placeholder)),
    )

    return df.drop(concat_placeholder)
